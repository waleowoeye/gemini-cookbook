{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waleowoeye/gemini-cookbook/blob/5-days-genai/Fine_tuning_a_custom_model_for_security.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2025 Google LLC."
      ],
      "metadata": {
        "id": "b6e13eef3f5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "cellView": "form",
        "id": "d6597b11df14",
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:25:52.183306Z",
          "iopub.execute_input": "2025-04-05T05:25:52.184498Z",
          "iopub.status.idle": "2025-04-05T05:25:52.190098Z",
          "shell.execute_reply.started": "2025-04-05T05:25:52.184439Z",
          "shell.execute_reply": "2025-04-05T05:25:52.188629Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 4 - Fine tuning a custom model\n",
        "\n",
        "Welcome back to the Kaggle 5-day Generative AI course!\n",
        "\n",
        "In this notebook you will use the Gemini API to fine-tune a custom, task-specific model. Fine-tuning can be used for a variety of tasks from classic NLP problems like entity extraction or summarisation, to creative tasks like stylised generation. You will fine-tune a model to classify the category a piece of text (a newsgroup post) into the category it belongs to (the newsgroup name).\n",
        "\n",
        "This codelab walks you tuning a model with the API. [AI Studio](https://aistudio.google.com/app/tune) also supports creating new tuned models directly in the web UI, allowing you to quickly create and monitor models using data from Google Sheets, Drive or your own files.\n",
        "\n",
        "**Note**: We recommend doing this codelab first today. There may be a period of waiting while the model tunes, so if you start with this one, you can try the other codelab while you wait."
      ],
      "metadata": {
        "id": "4KDIFPAL2EnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n",
        "!pip install -U -q \"google-genai==1.7.0\""
      ],
      "metadata": {
        "id": "9wafTyEH1_xF",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:25:52.194037Z",
          "iopub.execute_input": "2025-04-05T05:25:52.194573Z",
          "iopub.status.idle": "2025-04-05T05:26:04.877102Z",
          "shell.execute_reply.started": "2025-04-05T05:25:52.194525Z",
          "shell.execute_reply": "2025-04-05T05:26:04.875275Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccd8dbe-af0c-4733-8a36-d50f1b0f3a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "genai.__version__"
      ],
      "metadata": {
        "id": "T0CBG9xL2PvT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:04.880261Z",
          "iopub.execute_input": "2025-04-05T05:26:04.881474Z",
          "iopub.status.idle": "2025-04-05T05:26:04.889291Z",
          "shell.execute_reply.started": "2025-04-05T05:26:04.88141Z",
          "shell.execute_reply": "2025-04-05T05:26:04.887851Z"
        },
        "outputId": "9657ff2f-d2dd-4cd3-898a-b724bc75817c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.7.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
        "\n",
        "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
        "\n",
        "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
      ],
      "metadata": {
        "id": "P4bYX2T72ScK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "VuJPY3GK2SLZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:04.890946Z",
          "iopub.execute_input": "2025-04-05T05:26:04.891366Z",
          "iopub.status.idle": "2025-04-05T05:26:05.029338Z",
          "shell.execute_reply.started": "2025-04-05T05:26:04.89131Z",
          "shell.execute_reply": "2025-04-05T05:26:05.02809Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n",
        "\n",
        "![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)"
      ],
      "metadata": {
        "id": "25b2127c2052"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore available models\n",
        "\n",
        "You will be using the [`TunedModel.create`](https://ai.google.dev/api/tuning#method:-tunedmodels.create) API method to start the fine-tuning job and create your custom model. Find a model that supports it through the [`models.list`](https://ai.google.dev/api/models#method:-models.list) endpoint. You can also find more information about tuning models in [the model tuning docs](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python)."
      ],
      "metadata": {
        "id": "CqVA5QFO6n4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "    if \"createTunedModel\" in model.supported_actions:\n",
        "        print(model.name)"
      ],
      "metadata": {
        "id": "coEacWAB6o0G",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:05.043756Z",
          "iopub.execute_input": "2025-04-05T05:26:05.044173Z",
          "iopub.status.idle": "2025-04-05T05:26:05.111577Z",
          "shell.execute_reply.started": "2025-04-05T05:26:05.044118Z",
          "shell.execute_reply": "2025-04-05T05:26:05.110236Z"
        },
        "outputId": "f799d232-327b-424f-ff59-33fcb6d16afd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.5-flash-001-tuning\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Uploading the zip file"
      ],
      "metadata": {
        "id": "RA0oNIdoLuqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List uploaded files\n",
        "files = os.listdir('/content/secData')\n",
        "print(files)\n",
        "for file in files:\n",
        "  print(\"Uploaded File:\", file)\n",
        "'''\n",
        "secContent = files.upload()\n",
        "uploaded_files = list(secContent.keys())\n",
        "print(\"Uploaded Files:\", uploaded_files)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "iWzM11sCff9h",
        "outputId": "3e0a2447-afeb-4120-fe60-5c330b79288d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dns.txt.gz', 'flows.txt.gz', 'redteam.txt.gz']\n",
            "Uploaded File: dns.txt.gz\n",
            "Uploaded File: flows.txt.gz\n",
            "Uploaded File: redteam.txt.gz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nsecContent = files.upload()\\nuploaded_files = list(secContent.keys())\\nprint(\"Uploaded Files:\", uploaded_files)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unzipping it"
      ],
      "metadata": {
        "id": "dicoW0g-LnvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Make sure your folder exists\n",
        "os.makedirs(\"/content/data\", exist_ok=True)\n",
        "\n",
        "#Loop through each file\n",
        "for file in files:\n",
        "    # Define the path for the extracted file\n",
        "    output_file = os.path.join('/content/data', os.path.splitext(file)[0])  # remove .gz extension\n",
        "\n",
        "    # Check if the file has already been extracted\n",
        "    if not os.path.exists(output_file):  # Only unzip if it doesn't already exist\n",
        "        # Path to the original .gz file\n",
        "        gz_path = os.path.join('/content/secData', file)\n",
        "\n",
        "        # Check file integrity before attempting to extract\n",
        "        print(f\"Checking integrity of {gz_path}...\")  # Added for clarity\n",
        "        !gzip -t \"$gz_path\"  # Using shell command to check integrity\n",
        "        if os.WEXITSTATUS(os.system(f\"gzip -t '{gz_path}'\")) != 0:\n",
        "            print(f\"File {gz_path} is corrupted. Skipping extraction.\")\n",
        "            continue\n",
        "\n",
        "        # Unzip the file\n",
        "        with gzip.open(gz_path, 'rb') as f_in:\n",
        "            with open(output_file, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "        print(f\"Extracted: {output_file}\")\n",
        "    else:\n",
        "        print(f\"File {output_file} already exists, skipping extraction.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRtb-UgXLeA7",
        "outputId": "48bb2564-6373-4eb3-ed82-c53a7c14351d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/data/dns.txt already exists, skipping extraction.\n",
            "Checking integrity of /content/secData/flows.txt.gz...\n",
            "\n",
            "gzip: /content/secData/flows.txt.gz: unexpected end of file\n",
            "File /content/secData/flows.txt.gz is corrupted. Skipping extraction.\n",
            "File /content/data/redteam.txt already exists, skipping extraction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the dataset\n",
        "\n",
        "In this activity, we will use the cybersecurity dataset in (https://www.kaggle.com/code/markishere/day-2-classihttps://csr.lanl.gov/data/cyber1/). In this example you will use a fine-tuned Gemini model to achieve the same goal.\n",
        "\n",
        "The dataset contains 3 files\n",
        "['/content/data/redteam.txt', '/content/data/dns.txt', '/content/data/flows.txt']\n",
        "\n",
        "use the name of each file as a label, so\n",
        "- redteam.txt will be labeled redteam\n",
        "- dns.txt will be labeled dns and\n",
        "- flows.txt will be labeled flows\n",
        "\n",
        "However, I want to combine the 3 files first, then break them into train and test data\n",
        "\n",
        "Per the data set, with the first line as header\n",
        "flows.txt will contain\n",
        "time,duration,source computer,source port,destination computer,destination port,protocol,packet count,byte count\n",
        "1,9,C3090,N10471,C3420,N46,6,3,144\n",
        "1,9,C3538,N2600,C3371,N46,6,3,144\n",
        "2,0,C4316,N10199,C5030,443,6,2,92\n",
        "\n",
        "dns.txt will contain\n",
        "time,source computer,computer resolved\n",
        "31,C161,C2109\n",
        "35,C5642,C528\n",
        "38,C3380,C22841\n",
        "\n",
        "redteam.txt\n",
        "time,user@domain,source computer,destination computer\n",
        "151648,U748@DOM1,C17693,C728\n",
        "151993,U6115@DOM1,C17693,C1173\n",
        "153792,U636@DOM1,C17693,C294\n"
      ],
      "metadata": {
        "id": "XKntiFM44GM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what a single row looks like."
      ],
      "metadata": {
        "id": "ipafe6ptZFjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# File paths\n",
        "files = {\n",
        "    'redteam': '/content/data/redteam.txt',\n",
        "    'dns': '/content/data/dns.txt',\n",
        "    'flows': '/content/data/flows.txt'\n",
        "}\n",
        "\n",
        "# Create an empty DataFrame to combine data\n",
        "combined_data = pd.DataFrame()\n",
        "\n",
        "# Loop through the files and read each one\n",
        "for label, file_path in files.items():\n",
        "    # Read the file into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Add a new column with the label\n",
        "    df['label'] = label\n",
        "\n",
        "    # Append to the combined DataFrame\n",
        "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "\n",
        "# Show the combined data\n",
        "print(\"Combined Data Preview:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Split the combined data into train and test sets (80% train, 20% test)\n",
        "train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=42, stratify=combined_data['label'])\n",
        "\n",
        "# Show the split data\n",
        "print(\"\\nTrain Data Preview:\")\n",
        "print(train_data.head())\n",
        "\n",
        "print(\"\\nTest Data Preview:\")\n",
        "print(test_data.head())\n"
      ],
      "metadata": {
        "id": "tJzRRoE30yjy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "230683a8-a186-41c9-d181-f99d3fa71fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 3 fields in line 1071011, saw 6\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d5e4e579aed4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Read the file into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Add a new column with the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 1071011, saw 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total files: {len(file_paths)}\")\n",
        "print(f\"Training files: {len(train_files)}\")\n",
        "print(f\"Test files: {len(test_files)}\")\n",
        "\n",
        "print('*'*50)\n",
        "for idx, file in enumerate(train_files[:len(train_files)]):\n",
        "  print(f\"Train File {idx + 1}: {file}, Label: {train_labels[idx]}\")\n",
        "\n",
        "  if idx < len(train_files):\n",
        "    try:\n",
        "      with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        print(f\"\\n--- Preview of File {idx + 1} ---\")\n",
        "        print(f.read()[:500])  # First 500 characters\n",
        "    except Exception as e:\n",
        "      print(f\"Error reading file {file}: {e}\")\n",
        "\n",
        "'''\n",
        "for i in range(5):  # show first 5 test files\n",
        "  print(f\"Train File: {train_files[i]}, Label: {train_labels[i]}\")\n",
        "  # Show a few lines from some training files\n",
        "  for i in range(3):\n",
        "    with open(train_files[i], 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        print(f\"\\n--- File: {train_files[i]} ---\")\n",
        "        print(f.read()[:500])  # first 500 characters\n",
        "\n",
        "\n",
        "  print(f\"Test File: {test_files[i]}, Label: {test_labels[i]}\")\n",
        "  # Show a few lines from some test files\n",
        "  for i in range(3):\n",
        "    with open(test_files[i], 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        print(f\"\\n--- File: {test_files[i]} ---\")\n",
        "        print(f.read()[:500])  # first 500 characters\n",
        "\n",
        "'''\n",
        "print('*'*50)\n",
        "for idx, file in enumerate(test_files[:len(test_files)]):\n",
        "  print(f\"Train File {idx + 1}: {file}, Label: {test_labels[idx]}\")\n",
        "\n",
        "  if idx < len(test_files):\n",
        "    try:\n",
        "      with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        print(f\"\\n--- Preview of File {idx + 1} ---\")\n",
        "        print(f.read()[:500])  # First 500 characters\n",
        "    except Exception as e:\n",
        "      print(f\"Error reading file {file}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSJ1bIKXNCFe",
        "outputId": "a6498ce5-c1bd-4a1e-8dae-e775b50b62b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 3\n",
            "Training files: 2\n",
            "Test files: 1\n",
            "**************************************************\n",
            "Train File 1: /content/data/dns.txt, Label: data\n",
            "\n",
            "--- Preview of File 1 ---\n",
            "2,C4653,C5030\n",
            "2,C5782,C16712\n",
            "6,C1191,C419\n",
            "15,C3380,C22841\n",
            "18,C2436,C5030\n",
            "31,C161,C2109\n",
            "35,C5642,C528\n",
            "38,C3380,C22841\n",
            "42,C2428,C1065\n",
            "42,C2428,C2109\n",
            "42,C2428,C586\n",
            "43,C2428,C528\n",
            "44,C2428,C2109\n",
            "49,C5778,C706\n",
            "50,C2492,C457\n",
            "51,C1299,C2109\n",
            "53,C608,C706\n",
            "59,C2517,C1707\n",
            "62,C24436,C24436\n",
            "62,C26743,C26743\n",
            "62,C5778,C1685\n",
            "63,C3198,C1707\n",
            "63,C5778,C1685\n",
            "65,C1108,C706\n",
            "65,C2428,C457\n",
            "66,C2428,C457\n",
            "71,C4651,C1707\n",
            "73,C2428,C1685\n",
            "76,C18636,C1685\n",
            "76,C18636,C1707\n",
            "76,C3380,C22841\n",
            "80,C2303,C25571\n",
            "87,C2428,C625\n",
            "88,C2428,C\n",
            "Train File 2: /content/data/redteam.txt, Label: data\n",
            "\n",
            "--- Preview of File 2 ---\n",
            "150885,U620@DOM1,C17693,C1003\n",
            "151036,U748@DOM1,C17693,C305\n",
            "151648,U748@DOM1,C17693,C728\n",
            "151993,U6115@DOM1,C17693,C1173\n",
            "153792,U636@DOM1,C17693,C294\n",
            "155219,U748@DOM1,C17693,C5693\n",
            "155399,U748@DOM1,C17693,C152\n",
            "155460,U748@DOM1,C17693,C2341\n",
            "155591,U748@DOM1,C17693,C332\n",
            "156658,U748@DOM1,C17693,C4280\n",
            "210086,U748@DOM1,C18025,C1493\n",
            "210294,U748@DOM1,C18025,C1493\n",
            "210312,U748@DOM1,C18025,C1493\n",
            "218418,U748@DOM1,C17693,C504\n",
            "227052,U748@DOM1,C17693,C148\n",
            "227408,U748@DOM1,C17693,C148\n",
            "227520,U748@DOM1,C17693,C14\n",
            "**************************************************\n",
            "Train File 1: /content/data/flows.txt, Label: data\n",
            "\n",
            "--- Preview of File 1 ---\n",
            "1,0,C1065,389,C3799,N10451,6,10,5323\n",
            "1,0,C1423,N1136,C1707,N1,6,5,847\n",
            "1,0,C1423,N1142,C1707,N1,6,5,847\n",
            "1,0,C14909,N8191,C5720,2049,6,1,52\n",
            "1,0,C14909,N8192,C5720,2049,6,1,52\n",
            "1,0,C14909,N8193,C5720,2049,6,1,52\n",
            "1,0,C1707,N1,C1423,N1136,6,4,414\n",
            "1,0,C1707,N1,C1423,N1142,6,4,413\n",
            "1,0,C1707,N1,C925,N10487,6,4,414\n",
            "1,0,C1707,N1,C925,N10491,6,4,413\n",
            "1,0,C3587,N44,C528,N17,1,2,120\n",
            "1,0,C3799,N10451,C1065,389,6,12,3007\n",
            "1,0,C4879,N2369,C585,139,6,1,46\n",
            "1,0,C528,N17,C3587,N17,1,2,120\n",
            "1,0,C5720,2049,C14909,N8191,6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Training label distribution:\")\n",
        "print(Counter(train_labels))\n",
        "\n",
        "print(\"\\nTest label distribution:\")\n",
        "print(Counter(test_labels))"
      ],
      "metadata": {
        "id": "EtEXcdT39hCB",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:05.899119Z",
          "iopub.execute_input": "2025-04-05T05:26:05.900036Z",
          "iopub.status.idle": "2025-04-05T05:26:05.907224Z",
          "shell.execute_reply.started": "2025-04-05T05:26:05.899972Z",
          "shell.execute_reply": "2025-04-05T05:26:05.905627Z"
        },
        "outputId": "919693ff-bbe8-4e1c-ae37-2c6c8ee04162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training label distribution:\n",
            "Counter({'data': 2})\n",
            "\n",
            "Test label distribution:\n",
            "Counter({'data': 1})\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "You'll use the same pre-processing code you used for the custom model on day 2. This pre-processing removes personal information, which can be used to \"shortcut\" to known users of a forum, and formats the text to appear a bit more like regular text and less like a newsgroup post (e.g. by removing the mail headers). This normalisation allows the model to generalise to regular text and not over-depend on specific fields. If your input data is always going to be newsgroup posts, it may be helpful to leave this structure in place if they provide genuine signals."
      ],
      "metadata": {
        "id": "03lDs1O4ZQ0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import email\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def preprocess_newsgroup_row(data):\n",
        "    # Extract only the subject and body\n",
        "    msg = email.message_from_string(data)\n",
        "    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n",
        "    # Strip any remaining email addresses\n",
        "    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n",
        "    # Truncate the text to fit within the input limits\n",
        "    text = text[:40000]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_newsgroup_data(newsgroup_dataset):\n",
        "    # Put data points into dataframe\n",
        "    df = pd.DataFrame(\n",
        "        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n",
        "    )\n",
        "    # Clean up the text\n",
        "    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n",
        "    # Match label to target name index\n",
        "    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "IoNYTxpoZgB0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:05.90877Z",
          "iopub.execute_input": "2025-04-05T05:26:05.909241Z",
          "iopub.status.idle": "2025-04-05T05:26:05.922285Z",
          "shell.execute_reply.started": "2025-04-05T05:26:05.909202Z",
          "shell.execute_reply": "2025-04-05T05:26:05.921219Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to training and test datasets\n",
        "df_train = preprocess_newsgroup_data(newsgroups_train)\n",
        "df_test = preprocess_newsgroup_data(newsgroups_test)\n",
        "\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "kvOsUSRWaW4g",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:05.923836Z",
          "iopub.execute_input": "2025-04-05T05:26:05.924273Z",
          "iopub.status.idle": "2025-04-05T05:26:09.8222Z",
          "shell.execute_reply.started": "2025-04-05T05:26:05.924235Z",
          "shell.execute_reply": "2025-04-05T05:26:09.820955Z"
        },
        "outputId": "e146a908-db05-4dca-a9e6-44f16c343b03"
      },
      "outputs": [
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                Text  Label  \\\n0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...      1   \n4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n\n              Class Name  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WHAT car is this!?\\n\\n I was wondering if anyo...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SI Clock Poll - Final Call\\n\\nA fair number of...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PB questions...\\n\\nwell folks, my mac plus fin...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;...</td>\n      <td>14</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now sample the data. You will keep 50 rows for each category for training. Note that this is even fewer than the Keras example, as this technique (parameter-efficient fine-tuning, or PEFT) updates a relatively small number of parameters and does not require training a new model or updating the large model."
      ],
      "metadata": {
        "id": "XSKcj5WtadaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_data(df, num_samples, classes_to_keep):\n",
        "    # Sample rows, selecting num_samples of each Label.\n",
        "    df = (\n",
        "        df.groupby(\"Label\")[df.columns]\n",
        "        .apply(lambda x: x.sample(num_samples))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n",
        "    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "TRAIN_NUM_SAMPLES = 50\n",
        "TEST_NUM_SAMPLES = 10\n",
        "# Keep rec.* and sci.*\n",
        "CLASSES_TO_KEEP = \"^rec|^sci\"\n",
        "\n",
        "df_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\n",
        "df_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)"
      ],
      "metadata": {
        "id": "0t9Xu6X5akkt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:09.823625Z",
          "iopub.execute_input": "2025-04-05T05:26:09.824074Z",
          "iopub.status.idle": "2025-04-05T05:26:09.861784Z",
          "shell.execute_reply.started": "2025-04-05T05:26:09.824023Z",
          "shell.execute_reply": "2025-04-05T05:26:09.860559Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate baseline performance\n",
        "\n",
        "Before you start tuning a model, it's good practice to perform an evaluation on the available models to ensure you can measure how much the tuning helps.\n",
        "\n",
        "First identify a single sample row to use for visual inspection."
      ],
      "metadata": {
        "id": "J6F2XGhdBEPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0\n",
        "sample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx])\n",
        "sample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]\n",
        "\n",
        "print(sample_row)\n",
        "print('---')\n",
        "print('Label:', sample_label)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:09.865069Z",
          "iopub.execute_input": "2025-04-05T05:26:09.865518Z",
          "iopub.status.idle": "2025-04-05T05:26:09.872262Z",
          "shell.execute_reply.started": "2025-04-05T05:26:09.865478Z",
          "shell.execute_reply": "2025-04-05T05:26:09.87105Z"
        },
        "id": "LN-3mgAQBEPC",
        "outputId": "05a73d2a-eb4e-44e3-f8ba-9354abd15126"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Need info on 88-89 Bonneville\n\n\n I am a little confused on all of the models of the 88-89 bonnevilles.\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\ndifferences are far as features or performance. I am also curious to\nknow what the book value is for prefereably the 89 model. And how much\nless than book value can you usually get them for. In other words how\nmuch are they in demand this time of year. I have heard that the mid-spring\nearly summer is the best time to buy.\n\n\t\t\tNeil Gandler\n\n---\nLabel: rec.autos\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the text directly in as a prompt does not yield the desired results. The model will attempt to respond to the message."
      ],
      "metadata": {
        "id": "58R7rRDfBEPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-1.5-flash-001\", contents=sample_row)\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:09.873684Z",
          "iopub.execute_input": "2025-04-05T05:26:09.874017Z",
          "iopub.status.idle": "2025-04-05T05:26:13.310435Z",
          "shell.execute_reply.started": "2025-04-05T05:26:09.873983Z",
          "shell.execute_reply": "2025-04-05T05:26:13.309165Z"
        },
        "id": "43pxn4AkBEPC",
        "outputId": "4217544c-c789-4900-8719-fea690737254"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "##  Bonneville Models: Deciphering the Acronyms\n\nYou're right, the Bonneville model names can get confusing! Here's a breakdown of the 88-89 models and their key differences:\n\n**1. Bonneville:** This was the base model, often referred to as the \"standard\" Bonneville. It came with a 3.8L V6 engine and a variety of options like air conditioning, power steering, and automatic transmission.\n\n**2. Bonneville LE:** The LE (Luxury Edition) package added features like leather seating, power windows and locks, and a more upscale interior. It was still powered by the 3.8L V6.\n\n**3. Bonneville SE:** The SE (Special Edition) was similar to the LE but often included slightly different trim, interior colors, and wheels. It was also powered by the 3.8L V6.\n\n**4. Bonneville LSE:** The LSE (Luxury Sport Edition) combined features of the LE and SE with a more powerful 3.8L V6 engine and sport-tuned suspension. \n\n**5. Bonneville SSE:** The SSE (Super Sport Edition) was the top-of-the-line Bonneville. It had a 3.8L V6 engine with higher horsepower, a sport-tuned suspension, and distinctive styling cues. \n\n**6. Bonneville SSEi:** The SSEi (Super Sport Edition Injection) was a rare variant of the SSE, introduced in 1989. It featured a 3.8L V6 engine with a unique, higher-performance intake manifold and electronic fuel injection system. This gave it a significant horsepower boost over the regular SSE. \n\n**Book Values and Market Demand:**\n\nThe book value of an 89 Bonneville will vary significantly depending on its condition, mileage, options, and location.  You can find these values using online resources like Kelley Blue Book (KBB) or Edmunds.  \n\n**Here's a general idea of what to expect:**\n\n* **Good condition, low mileage:** These cars can fetch close to their book value, but they are rarer.\n* **Average condition, average mileage:** Expect to pay somewhere around 70-80% of book value.\n* **High mileage, needing repairs:**  Prices will be much lower, potentially 50% or less than book value.\n\n**Spring/Summer Buying:**\n\nWhile it's true that the mid-spring to early summer season tends to be a good time to buy a car (dealers are trying to clear inventory), the Bonneville market is generally slow due to its age.  \n\n**Tips for Buying:**\n\n* **Research thoroughly:** Compare prices and features across different sellers. \n* **Check for maintenance records:**  Make sure the car has been properly maintained.\n* **Get a pre-purchase inspection:**  Have a trusted mechanic check the car before you buy it.\n* **Negotiate:** Don't be afraid to haggle for a better price, especially for a car in less-than-perfect condition.\n\nRemember, the best deal is one that makes you happy and suits your budget.  Good luck with your search! \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the prompt engineering techniques you have learned this week to induce the model to perform the desired task. Try some of your own ideas and see what is effective, or check out the following cells for different approaches. Note that they have different levels of effectiveness!"
      ],
      "metadata": {
        "id": "YatXs9OgBEPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the model directly in a zero-shot prompt.\n",
        "\n",
        "prompt = \"From what newsgroup does the following message originate?\"\n",
        "baseline_response = client.models.generate_content(\n",
        "    model=\"gemini-1.5-flash-001\",\n",
        "    contents=[prompt, sample_row])\n",
        "print(baseline_response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:13.31225Z",
          "iopub.execute_input": "2025-04-05T05:26:13.3127Z",
          "iopub.status.idle": "2025-04-05T05:26:14.754614Z",
          "shell.execute_reply.started": "2025-04-05T05:26:13.312651Z",
          "shell.execute_reply": "2025-04-05T05:26:14.753174Z"
        },
        "id": "Zrg0-mC2BEPD",
        "outputId": "a5fa26e1-87f7-47d2-cbda-657beaaabe0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The message most likely originates from a newsgroup focused on **Buick vehicles**, specifically the **Bonneville model**.\n\nHere's why:\n\n* **Subject:** The message clearly discusses the 1988-1989 Buick Bonneville, a popular car model during that time.\n* **Terminology:** The message uses specific Bonneville trim levels like LE, SE, LSE, SSE, and SSEi, indicating a focus on this specific model.\n* **Information Requested:**  The author seeks information about the differences between these trim levels and their value. This suggests an interest in buying or owning a Bonneville.\n\nBased on these clues, a likely newsgroup would be:\n\n* **alt.autos.buick:** A newsgroup dedicated to all things Buick.\n* **rec.autos.buick:** Another newsgroup focused on Buick cars and related discussions.\n* **alt.autos.bonneville:** A more specific newsgroup devoted to the Buick Bonneville model. \n\nYou might find similar discussions on car enthusiast forums or online communities focused on classic cars and Buick models. \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique still produces quite a verbose response. You could try and parse out the relevant text, or refine the prompt even further."
      ],
      "metadata": {
        "id": "9CcwbBduBEPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "# You can use a system instruction to do more direct prompting, and get a\n",
        "# more succinct answer.\n",
        "\n",
        "system_instruct = \"\"\"\n",
        "You are a classification service. You will be passed input that represents\n",
        "a newsgroup post and you must respond with the newsgroup from which the post\n",
        "originates.\n",
        "\"\"\"\n",
        "\n",
        "# Define a helper to retry when per-minute quota is reached.\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "# If you want to evaluate your own technique, replace this body of this function\n",
        "# with your model, prompt and other code and return the predicted answer.\n",
        "@retry.Retry(predicate=is_retriable)\n",
        "def predict_label(post: str) -> str:\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-1.5-flash-001\",\n",
        "        config=types.GenerateContentConfig(\n",
        "            system_instruction=system_instruct),\n",
        "        contents=post)\n",
        "\n",
        "    rc = response.candidates[0]\n",
        "\n",
        "    # Any errors, filters, recitation, etc we can mark as a general error\n",
        "    if rc.finish_reason.name != \"STOP\":\n",
        "        return \"(error)\"\n",
        "    else:\n",
        "        # Clean up the response.\n",
        "        return response.text.strip()\n",
        "\n",
        "\n",
        "prediction = predict_label(sample_row)\n",
        "\n",
        "print(prediction)\n",
        "print()\n",
        "print(\"Correct!\" if prediction == sample_label else \"Incorrect.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:14.756309Z",
          "iopub.execute_input": "2025-04-05T05:26:14.756663Z",
          "iopub.status.idle": "2025-04-05T05:26:15.034554Z",
          "shell.execute_reply.started": "2025-04-05T05:26:14.756627Z",
          "shell.execute_reply": "2025-04-05T05:26:15.033201Z"
        },
        "id": "5LDbRjMvBEPD",
        "outputId": "75570f7a-d0a0-4e48-e6b9-1e1cb261cd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "rec.autos.pontiac\n\nIncorrect.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run a short evaluation using the function defined above. The test set is further sampled to ensure the experiment runs smoothly on the API's free tier. In practice you would evaluate over the whole set."
      ],
      "metadata": {
        "id": "RbC5B1IKBEPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from tqdm.rich import tqdm as tqdmr\n",
        "import warnings\n",
        "\n",
        "# Enable tqdm features on Pandas.\n",
        "tqdmr.pandas()\n",
        "\n",
        "# But suppress the experimental warning\n",
        "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n",
        "\n",
        "\n",
        "# Further sample the test data to be mindful of the free-tier quota.\n",
        "df_baseline_eval = sample_data(df_test, 2, '.*')\n",
        "\n",
        "# Make predictions using the sampled data.\n",
        "df_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)\n",
        "\n",
        "# And calculate the accuracy.\n",
        "accuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum() / len(df_baseline_eval)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:15.035748Z",
          "iopub.execute_input": "2025-04-05T05:26:15.03608Z",
          "iopub.status.idle": "2025-04-05T05:26:28.935943Z",
          "shell.execute_reply.started": "2025-04-05T05:26:15.036045Z",
          "shell.execute_reply": "2025-04-05T05:26:28.934701Z"
        },
        "id": "2LjsDzOPBEPE",
        "outputId": "0d2bb5b4-2057-4840-e4f6-8ad77ef72181",
        "colab": {
          "referenced_widgets": [
            "89d0c4b980c548bbb0ff12b57fa91386"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Output()",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89d0c4b980c548bbb0ff12b57fa91386"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Accuracy: 25.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now take a look at the dataframe to compare the predictions with the labels."
      ],
      "metadata": {
        "id": "pTQq7p_0BEPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_baseline_eval"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:26:28.937315Z",
          "iopub.execute_input": "2025-04-05T05:26:28.937639Z",
          "iopub.status.idle": "2025-04-05T05:26:28.952416Z",
          "shell.execute_reply.started": "2025-04-05T05:26:28.937607Z",
          "shell.execute_reply": "2025-04-05T05:26:28.95071Z"
        },
        "id": "kdxqaYWrBEPE",
        "outputId": "4b8cf4aa-5803-4170-ef66-3138a0bb4f0c"
      },
      "outputs": [
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                 Text  Label  \\\n0   Drag Coefficients\\n\\nCould someone explain how...      7   \n1   Alternative Fuel Vehicles\\n\\n\\nHere is a press...      7   \n2   Re: Test ride on a GTS1000\\n\\nIn article ,  (T...      8   \n3   Re: Bra... bra... brazing  (Was: For  )\\n\\nOld...      8   \n4   Re: Twins Update 4-22\\n\\n (Paul R Krueger) wri...      9   \n5   Re: Jack Morris\\n\\nIn article <>  (Roger Mayna...      9   \n6   Re: ESPN f*ck up\\n\\nIn article <>  (Matt Coohi...     10   \n7   Re: Lemieux's Getting the Hart... Jeez I hope ...     10   \n8   Re: Off the shelf cheap DES keyseach machine (...     11   \n9   Re: Overreacting (was Re: Once tapped, your co...     11   \n10  Re: Power, signal surges in home...\\n\\n (David...     12   \n11  Re: How to the disks copy protected.\\n\\nIn art...     12   \n12  Re: Antihistamine for sleep aid\\n\\nIn article ...     13   \n13  Re: OCD\\n\\n\\nThis is to followup my previous r...     13   \n14  Re: Who is Henry Spencer anyway?\\n\\nOn Sat, 15...     14   \n15  Re: Gamma Ray Bursters. WHere are they.\\n\\n (P...     14   \n\n            Class Name                   Prediction  \n0            rec.autos     comp.sys.ibm.pc.hardware  \n1            rec.autos  alt.autos.alternative-fuels  \n2      rec.motorcycles              rec.motorcycles  \n3      rec.motorcycles              rec.motorcycles  \n4   rec.sport.baseball      rec.sports.baseball.mlb  \n5   rec.sport.baseball           rec.sport.baseball  \n6     rec.sport.hockey            rec.sports.hockey  \n7     rec.sport.hockey             rec.sport.hockey  \n8            sci.crypt                      (error)  \n9            sci.crypt         alt.politics.clinton  \n10     sci.electronics                      (error)  \n11     sci.electronics     comp.os.msdos.programmer  \n12             sci.med                      (error)  \n13             sci.med              alt.support.ocd  \n14           sci.space                 rec.aviation  \n15           sci.space                    sci.astro  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Drag Coefficients\\n\\nCould someone explain how...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>comp.sys.ibm.pc.hardware</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Alternative Fuel Vehicles\\n\\n\\nHere is a press...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>alt.autos.alternative-fuels</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Re: Test ride on a GTS1000\\n\\nIn article ,  (T...</td>\n      <td>8</td>\n      <td>rec.motorcycles</td>\n      <td>rec.motorcycles</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Bra... bra... brazing  (Was: For  )\\n\\nOld...</td>\n      <td>8</td>\n      <td>rec.motorcycles</td>\n      <td>rec.motorcycles</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Twins Update 4-22\\n\\n (Paul R Krueger) wri...</td>\n      <td>9</td>\n      <td>rec.sport.baseball</td>\n      <td>rec.sports.baseball.mlb</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Re: Jack Morris\\n\\nIn article &lt;&gt;  (Roger Mayna...</td>\n      <td>9</td>\n      <td>rec.sport.baseball</td>\n      <td>rec.sport.baseball</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Re: ESPN f*ck up\\n\\nIn article &lt;&gt;  (Matt Coohi...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n      <td>rec.sports.hockey</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Re: Lemieux's Getting the Hart... Jeez I hope ...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n      <td>rec.sport.hockey</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Re: Off the shelf cheap DES keyseach machine (...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>(error)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Re: Overreacting (was Re: Once tapped, your co...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>alt.politics.clinton</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Re: Power, signal surges in home...\\n\\n (David...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n      <td>(error)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Re: How to the disks copy protected.\\n\\nIn art...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n      <td>comp.os.msdos.programmer</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Re: Antihistamine for sleep aid\\n\\nIn article ...</td>\n      <td>13</td>\n      <td>sci.med</td>\n      <td>(error)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Re: OCD\\n\\n\\nThis is to followup my previous r...</td>\n      <td>13</td>\n      <td>sci.med</td>\n      <td>alt.support.ocd</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Re: Who is Henry Spencer anyway?\\n\\nOn Sat, 15...</td>\n      <td>14</td>\n      <td>sci.space</td>\n      <td>rec.aviation</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Re: Gamma Ray Bursters. WHere are they.\\n\\n (P...</td>\n      <td>14</td>\n      <td>sci.space</td>\n      <td>sci.astro</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tune a custom model\n",
        "\n",
        "In this example you'll use tuning to create a model that requires no prompting or system instructions and outputs succinct text from the classes you provide in the training data.\n",
        "\n",
        "The data contains both input text (the processed posts) and output text (the category, or newsgroup), that you can use to start tuning a model.\n",
        "\n",
        "When calling `tune()`, you can specify model tuning hyperparameters too:\n",
        " - `epoch_count`: defines how many times to loop through the data,\n",
        " - `batch_size`: defines how many rows to process in a single step, and\n",
        " - `learning_rate`: defines the scaling factor for updating model weights at each step.\n",
        "\n",
        "You can also choose to omit them and use the defaults. [Learn more](https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters) about these parameters and how they work. For this example these parameters were selected by running some tuning jobs and selecting parameters that converged efficiently.\n",
        "\n",
        "This example will start a new tuning job, but only if one does not already exist. This allows you to leave this codelab and come back later - re-running this step will find your last model."
      ],
      "metadata": {
        "id": "Ok7ugrLzcghX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Iterable\n",
        "import random\n",
        "\n",
        "\n",
        "# Convert the data frame into a dataset suitable for tuning.\n",
        "input_data = {'examples':\n",
        "    df_train[['Text', 'Class Name']]\n",
        "      .rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n",
        "      .to_dict(orient='records')\n",
        " }\n",
        "\n",
        "# If you are re-running this lab, add your model_id here.\n",
        "model_id = None\n",
        "\n",
        "# Or try and find a recent tuning job.\n",
        "if not model_id:\n",
        "  queued_model = None\n",
        "  # Newest models first.\n",
        "  for m in reversed(client.tunings.list()):\n",
        "    # Only look at newsgroup classification models.\n",
        "    if m.name.startswith('tunedModels/newsgroup-classification-model'):\n",
        "      # If there is a completed model, use the first (newest) one.\n",
        "      if m.state.name == 'JOB_STATE_SUCCEEDED':\n",
        "        model_id = m.name\n",
        "        print('Found existing tuned model to reuse.')\n",
        "        break\n",
        "\n",
        "      elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n",
        "        # If there's a model still queued, remember the most recent one.\n",
        "        queued_model = m.name\n",
        "  else:\n",
        "    if queued_model:\n",
        "      model_id = queued_model\n",
        "      print('Found queued model, still waiting.')\n",
        "\n",
        "\n",
        "# Upload the training data and queue the tuning job.\n",
        "if not model_id:\n",
        "    tuning_op = client.tunings.tune(\n",
        "        base_model=\"models/gemini-1.5-flash-001-tuning\",\n",
        "        training_dataset=input_data,\n",
        "        config=types.CreateTuningJobConfig(\n",
        "            tuned_model_display_name=\"Newsgroup classification model\",\n",
        "            batch_size=16,\n",
        "            epoch_count=2,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    print(tuning_op.state)\n",
        "    model_id = tuning_op.name\n",
        "\n",
        "print(model_id)"
      ],
      "metadata": {
        "id": "pWOZlspfY8dV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:38:19.391048Z",
          "iopub.status.idle": "2025-04-05T05:38:19.391677Z",
          "shell.execute_reply.started": "2025-04-05T05:38:19.391379Z",
          "shell.execute_reply": "2025-04-05T05:38:19.391409Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has created a tuning job that will run in the background. To inspect the progress of the tuning job, run this cell to plot the current status and loss curve. Once the status reaches `ACTIVE`, tuning is complete and the model is ready to use.\n",
        "\n",
        "Tuning jobs are queued, so it may look like no training steps have been taken initially but it will progress. Tuning can take anywhere from a few minutes to multiple hours, depending on factors like your dataset size and how busy the tuning infrastrature is. Why not treat yourself to a nice cup of tea while you wait, or come and say \"Hi!\" in the group [Discord](https://discord.com/invite/kaggle).\n",
        "\n",
        "It is safe to stop this cell at any point. It will not stop the tuning job.\n",
        "\n",
        "**IMPORTANT**: Due to the high volume of users doing this course, tuning jobs may be queued for many hours. Take a note of your tuned model ID above (`tunedModels/...`) so you can come back to it tomorrow. In the meantime, check out the [Search grounding](https://www.kaggle.com/code/markishere/day-4-google-search-grounding/) codelab. If you want to try tuning a local LLM, check out [the fine-tuning guides for tuning a Gemma model](https://ai.google.dev/gemma/docs/tune)."
      ],
      "metadata": {
        "id": "NQ3YZ2MBubCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "MAX_WAIT = datetime.timedelta(minutes=10)\n",
        "\n",
        "while not (tuned_model := client.tunings.get(name=model_id)).has_ended:\n",
        "\n",
        "    print(tuned_model.state)\n",
        "    time.sleep(60)\n",
        "\n",
        "    # Don't wait too long. Use a public model if this is going to take a while.\n",
        "    if datetime.datetime.now(datetime.timezone.utc) - tuned_model.create_time > MAX_WAIT:\n",
        "        print(\"Taking a shortcut, using a previously prepared model.\")\n",
        "        model_id = \"tunedModels/newsgroup-classification-model-ltenbi1b\"\n",
        "        tuned_model = client.tunings.get(name=model_id)\n",
        "        break\n",
        "\n",
        "\n",
        "print(f\"Done! The model state is: {tuned_model.state.name}\")\n",
        "\n",
        "if not tuned_model.has_succeeded and tuned_model.error:\n",
        "    print(\"Error:\", tuned_model.error)"
      ],
      "metadata": {
        "id": "c4ef5f13692d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:38:19.393027Z",
          "iopub.status.idle": "2025-04-05T05:38:19.393529Z",
          "shell.execute_reply.started": "2025-04-05T05:38:19.39332Z",
          "shell.execute_reply": "2025-04-05T05:38:19.393346Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the new model\n",
        "\n",
        "Now that you have a tuned model, try it out with custom data. You use the same API as a normal Gemini API interaction, but you specify your new model as the model name, which will start with `tunedModels/`."
      ],
      "metadata": {
        "id": "9-qiIdK4u80z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"\"\"\n",
        "First-timer looking to get out of here.\n",
        "\n",
        "Hi, I'm writing about my interest in travelling to the outer limits!\n",
        "\n",
        "What kind of craft can I buy? What is easiest to access from this 3rd rock?\n",
        "\n",
        "Let me know how to do that please.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_id, contents=new_text)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "hyO2-MXLvM6a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:38:19.394897Z",
          "iopub.status.idle": "2025-04-05T05:38:19.395347Z",
          "shell.execute_reply.started": "2025-04-05T05:38:19.395151Z",
          "shell.execute_reply": "2025-04-05T05:38:19.395178Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "You can see that the model outputs labels that correspond to those in the training data, and without any system instructions or prompting, which is already a great improvement. Now see how well it performs on the test set.\n",
        "\n",
        "Note that there is no parallelism in this example; classifying the test sub-set will take a few minutes."
      ],
      "metadata": {
        "id": "xajLek9DySH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@retry.Retry(predicate=is_retriable)\n",
        "def classify_text(text: str) -> str:\n",
        "    \"\"\"Classify the provided text into a known newsgroup.\"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model=model_id, contents=text)\n",
        "    rc = response.candidates[0]\n",
        "\n",
        "    # Any errors, filters, recitation, etc we can mark as a general error\n",
        "    if rc.finish_reason.name != \"STOP\":\n",
        "        return \"(error)\"\n",
        "    else:\n",
        "        return rc.content.parts[0].text\n",
        "\n",
        "\n",
        "# The sampling here is just to minimise your quota usage. If you can, you should\n",
        "# evaluate the whole test set with `df_model_eval = df_test.copy()`.\n",
        "df_model_eval = sample_data(df_test, 4, '.*')\n",
        "\n",
        "df_model_eval[\"Prediction\"] = df_model_eval[\"Text\"].progress_apply(classify_text)\n",
        "\n",
        "accuracy = (df_model_eval[\"Class Name\"] == df_model_eval[\"Prediction\"]).sum() / len(df_model_eval)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "6T2Y3ZApvbMw",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:38:19.39758Z",
          "iopub.status.idle": "2025-04-05T05:38:19.398159Z",
          "shell.execute_reply.started": "2025-04-05T05:38:19.397858Z",
          "shell.execute_reply": "2025-04-05T05:38:19.397888Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare token usage\n",
        "\n",
        "AI Studio and the Gemini API provide model tuning at no cost, however normal limits and charges apply for *use* of a tuned model.\n",
        "\n",
        "The size of the input prompt and other generation config like system instructions, as well as the number of generated output tokens, all contribute to the overall cost of a request."
      ],
      "metadata": {
        "id": "ML3okG_vBEPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the *input* cost of the baseline model with system instructions.\n",
        "sysint_tokens = client.models.count_tokens(\n",
        "    model='gemini-1.5-flash-001', contents=[system_instruct, sample_row]\n",
        ").total_tokens\n",
        "print(f'System instructed baseline model: {sysint_tokens} (input)')\n",
        "\n",
        "# Calculate the input cost of the tuned model.\n",
        "tuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens\n",
        "print(f'Tuned model: {tuned_tokens} (input)')\n",
        "\n",
        "savings = (sysint_tokens - tuned_tokens) / tuned_tokens\n",
        "print(f'Token savings: {savings:.2%}')  # Note that this is only n=1."
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:38:19.399497Z",
          "iopub.status.idle": "2025-04-05T05:38:19.40003Z",
          "shell.execute_reply.started": "2025-04-05T05:38:19.399765Z",
          "shell.execute_reply": "2025-04-05T05:38:19.399792Z"
        },
        "id": "fl3NxarkBEPM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The earlier verbose model also produced more output tokens than needed for this task."
      ],
      "metadata": {
        "id": "uOjkulLQBEPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_token_output = baseline_response.usage_metadata.candidates_token_count\n",
        "print('Baseline (verbose) output tokens:', baseline_token_output)\n",
        "\n",
        "tuned_model_output = client.models.generate_content(\n",
        "    model=model_id, contents=sample_row)\n",
        "tuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count\n",
        "print('Tuned output tokens:', tuned_tokens_output)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T05:38:19.402328Z",
          "iopub.status.idle": "2025-04-05T05:38:19.402937Z",
          "shell.execute_reply.started": "2025-04-05T05:38:19.402652Z",
          "shell.execute_reply": "2025-04-05T05:38:19.402682Z"
        },
        "id": "MNY4A0u_BEPN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "\n",
        "Now that you have tuned a classification model, try some other tasks, like tuning a model to respond with a specific tone or style using hand-written examples (or even generated examples!). Kaggle hosts [a number of datasets](https://www.kaggle.com/datasets) you can try out.\n",
        "\n",
        "Learn about [when supervised fine-tuning is most effective](https://cloud.google.com/blog/products/ai-machine-learning/supervised-fine-tuning-for-gemini-llm).\n",
        "\n",
        "And check out the [fine-tuning tutorial](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?hl=en&lang=python) for another example that shows a tuned model extending beyond the training data to new, unseen inputs.\n",
        "\n",
        "*- [Mark McD](https://linktr.ee/markmcd)*"
      ],
      "metadata": {
        "id": "6c1204a5d0ab"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "NytEW6v_BEPN"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}